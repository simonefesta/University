### Backpropagation NNs multilivello

Non ci limitiamo a scalari, ma vettori e matrici,
Abbiamo due versioni:

- forward pass: dall'input calcoliamo tutte le attivazioni dei vari livelli, fino a calcolare il valore della funzione costo j.
  itero su tutti i livelli (anche quelli di uscita).
  $h_0 \leftarrow x$, alla fine abbiamo $y \leftarrow h^{(L)}$

- backward phase: gradiente funzione costo rispetto a tutti i parametri della rete. Più complicato perchè vado a prendere scorciatoie, che ottimizzano l'esecuzione.
  vediamo le righe:
  
  1) g è gradiente della funzione loss rispetto ad y. Ciò che facciamo è derivata di L rispetto ad ogni neurone di uscita y.
  
  2) iteriamo dall'ultimo livello ed andiamo indietro.
     ![](/home/festinho/.var/app/com.github.marktext.marktext/config/marktext/images/2023-10-27-11-54-37-IMG_0006.jpg)
  
  3) ho applicato la chain rule, ed aggiorno *g*, prima era gradiente funzione costo rispetto alle uscite, un elemento per ogni uscita. Faccio il prodotto elemento per elemento, non è vettoriale.
     Se usassi la *softmax* invece della *sigmoid* ad esempio, calcolo il pre-activation 'a' (l'altra volta l'abbiamo chiamato 'z') e calcolo $\frac{e^{z_i}}{{1+e^{z_i}}}$, la riga 3 non da un risultato corretto. Il calcolo sarebbe diverso, ma comunque i framework già implementano ciò.
  
  4) $J(\theta)=L(y,\theta)+\lambda\Omega(\theta)$
     
     $a^{(2)}$=  $W^{(z)} h^{(1)}+b^{(2)}$, con W matrice 2x3, devo avere tanti elementi quanti sono quelli della matrice W.
  
  5) /
  
  6) ora facciamo back-propagazione rispetto al gradiente.
     
  
  Quindi siamo partiti calcolando gradiente rispetto le uscite, e poi iterando dall'ultimo fino al primo livello nascosto. Passo da 'h' ad 'a' applicando la chain rule. La riga 6 propaga all'indietro questi gradienti, g lo uso per tale propagazione. 
  
  esercizio p.28
  sigmoid = attivazione logistica
  dobbiamo applicare backpropr.
  svolgimento:
  
  inizio in avanti, calcolo uscita primo livello nascosto, calcolo pre-activation value $a^{(1)} = W^{(1)}x+b^{(1)}$ a cui devo però attivare la funzione di attivazione per avere le uscite di questo primo livello nascosto.
  $h^{(1)}=\sigma(a^{(1)})$ applicando la sigmoid.
  per $a^{(2)}$ avrò due valori in uscita, applico le formule e poi calcolo y, che non ha valori molto buoni.
  Calcolo la loss.
  Adesso passiamo al backward pass:
  inizializziamo variabile di appoggio $g$, rispetto a valore di uscita $y$. Poi inizia iterazione da ultimo livello fino al primo.
  derivo funzione di applicazione e moltiplico per 'g'. Prodotto elemento per elemento. Esce una matrice, e ciascun elemento è derivata parziale loss rispetto ad uno di quegli elementi. Manca una iterazione da fare noi.
  
  
  ### notebook - Weight update aka SGD
  
  Alpha mi dice di quanto muovermi.
  
  
  ### Computational Demand
  
  il costo è proporzionale ad O(n), cioè alle variabili in gioco.
  Backward è o(n^2) perchè considero gli archi.
  I pesi totali nella rete sono: L livelli, m parametri, allora $L \cdot m^2$ sia backwar che forward.



### Generalizing Backprop: Input Batches

noi lavoriamo con mini batch, invece di vettore x ho matrice x, dove ogni riga è una riga diversa di training. Come cambia il training? ...
batch piccoli garantiscono miglior convergenza, in particolare si usano potenze di 2, perchè? hw parallelizza cose.

### Symbol to symbol differentiation

usiamo grafi per rappresentare simboli in gioco (variabili senza valore associato.) approccio alternativo è symbol-2-symbol, ovvero estendo grafo aggiungendo nodi con delle operazioni necessarie per calcolare cose.
il vantaggio è che se alg è efficiente in forward, anche la differenziazione è efficiente. tuttavia visto il grafo sembra che io sia "forzato" dal conoscere le derivate f', sennò non vado avanti. Possiamo generalizzare ancora di più ad esempio con backprop(), funzione che usa la derivata della funzione calcolata per propagare il gradiente. Abbiamo si delle cose built-in(), ma se voglio invetarmi nuova f. attivazione posso farlo senza problemi, bensì definisco nuova operazione, dico la derivata, e posso usarla in un grafo più complesso.



### example

esistono librerie più avanzate di scikit-learn, che non usa gpu, ed è meno flessibile, ad esempio non posso avere reti più complesse di quella messa in foto.


